import numpy as np
import mujoco
import gym
from gym import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import torch.nn as nn
import warnings
import torch
import mujoco.viewer
import os
from scipy.spatial.transform import Rotation as Rotation
from stable_baselines3.common.evaluation import evaluate_policy
import argparse
import cv2
import glfw

# å¿½ç•¥ç‰¹å®šè­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="stable_baselines3.common.on_policy_algorithm")

class PiperEnv(gym.Env):
    def __init__(self, render=False):
        super(PiperEnv, self).__init__()
        # è·å–å½“å‰è„šæœ¬æ–‡ä»¶æ‰€åœ¨ç›®å½•
        script_dir = os.path.dirname(os.path.realpath(__file__))
        # æ„é€  scene.xml çš„å®Œæ•´è·¯å¾„
        xml_path = os.path.join(script_dir, '../..', 'model_assets', 'piper_on_desk', 'scene.xml')
        # åŠ è½½æ¨¡å‹
        self.model = mujoco.MjModel.from_xml_path(xml_path)
        self.data = mujoco.MjData(self.model)
        self.end_effector_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, 'link6')
        self.render_mode = render
        if self.render_mode:
            self.handle = mujoco.viewer.launch_passive(self.model, self.data)
            self.handle.cam.distance = 3
            self.handle.cam.azimuth = 0
            self.handle.cam.elevation = -30
        else:
            self.handle = None

        # å„å…³èŠ‚è¿åŠ¨é™ä½
        self.joint_limits = np.array([
            (-2.618, 2.618),
            (0, 3.14),
            (-2.697, 0),
            (-1.832, 1.832),
            (-1.22, 1.22),
            (-3.14, 3.14),
        ])

        # åŠ¨ä½œç©ºé—´ï¼Œ6ä¸ªå…³èŠ‚
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(6,))
        # è§‚æµ‹ç©ºé—´ï¼ŒåŒ…å«æœ«ç«¯ä½å§¿å’Œç›®æ ‡ä½å§¿
        self.observation_space = spaces.Dict({
            "wrist": spaces.Box(low=0, high=255, shape=(3, 480, 640), dtype=np.uint8),
            "3rd": spaces.Box(low=0, high=255, shape=(3, 480, 640), dtype=np.uint8) 
        })
        self.np_random = None   
        self.step_number = 0

        #workspace limit of robot
        self.workspace_limits = {
            'x' : (0.1, 0.7),
            'y' : (-0.7, 0.7),
            'z' : (0.1, 0.7)
        }

        self.goal_reached = False
        self._reset_noise_scale = 0.0

        # åˆå§‹åŒ–ç›®æ ‡ pose
        self.goal_pos = None
        self.goal_quat = None
        self.goal_angle = None
        self._set_goal_pose()

        self.episode_len = 200
        self.init_qpos = np.zeros(6)
        self.init_qvel = np.zeros(6)

        # âœ… æ­£ç¡®ä½¿ç”¨ glfwï¼ˆæ¨¡å—è°ƒç”¨ï¼Œä¸åŠ  self.ï¼‰
        glfw.init()
        glfw.window_hint(glfw.VISIBLE, glfw.FALSE)
        self.window = glfw.create_window(640, 480, "offscreen", None, None)
        glfw.make_context_current(self.window)

        self.camera = mujoco.MjvCamera()
        self.camera.type = mujoco.mjtCamera.mjCAMERA_FIXED 
        self.scene = mujoco.MjvScene(self.model, maxgeom=1000)
        self.context = mujoco.MjrContext(self.model, mujoco.mjtFontScale.mjFONTSCALE_150)
        mujoco.mjr_setBuffer(mujoco.mjtFramebuffer.mjFB_OFFSCREEN, self.context)
    
    #set random goal position for cartesian space
    def _label_goal_pose(self, position, quat_wxyz):
        """
        è®¾ç½®ç›®æ ‡ä½å§¿ï¼ˆä½ç½® + å§¿æ€ï¼‰

        Args:
            position: ç›®æ ‡çš„ä½ç½®ï¼Œ(x, y, z)ï¼Œç±»å‹ä¸º numpy.ndarray æˆ– listã€‚
            quat_wxyz: ç›®æ ‡çš„å§¿æ€ï¼Œå››å…ƒæ•° (w, x, y, z)ï¼Œç±»å‹ä¸º numpy.ndarray æˆ– listã€‚
        """
        ## ====== è®¾ç½® target çš„ä½å§¿ ======
        goal_body_name = "target"
        goal_body_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, goal_body_name)

        if goal_body_id == -1:
            raise ValueError(f"Body named '{goal_body_name}' not found in the model.")

        # è·å– joint ID å’Œ qpos èµ·å§‹ç´¢å¼•
        goal_joint_id = self.model.body_jntadr[goal_body_id]
        goal_qposadr = self.model.jnt_qposadr[goal_joint_id]

        # è®¾ç½®ä½å§¿
        if goal_qposadr + 7 <= self.model.nq:
            self.data.qpos[goal_qposadr     : goal_qposadr + 3] = position
            self.data.qpos[goal_qposadr + 3 : goal_qposadr + 7] = quat_wxyz
        else:
            print("[è­¦å‘Š] target çš„ qpos ç´¢å¼•è¶Šç•Œæˆ– joint è®¾ç½®æœ‰è¯¯")

    def _get_site_pos_ori(self, site_name: str) -> tuple[np.ndarray, np.ndarray]:
        site_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE, site_name)
        if site_id == -1:
            raise ValueError(f"æœªæ‰¾åˆ°åä¸º '{site_name}' çš„site")

        # ä½ç½®
        position = np.array(self.data.site(site_id).xpos)        # shape (3,)

        # æ–¹å‘ï¼šMuJoCo å·²å­˜æˆ9å…ƒç´ å‘é‡ï¼Œæ— éœ€reshape
        xmat = np.array(self.data.site(site_id).xmat)            # shape (9,)
        quaternion = np.zeros(4)
        mujoco.mju_mat2Quat(quaternion, xmat)                    # [w, x, y, z]

        return position, quaternion



    def _set_goal_pose(self):
        while True:
            # piper xml é‡Œå®šä¹‰çš„ 6 ä¸ªå…³èŠ‚è§’åå­—
            joints = ["joint1", "joint2", "joint3", "joint4", "joint5", "joint6"]
            # é€šè¿‡éšæœºåœ¨å…³èŠ‚ç©ºé—´å†…é‡‡æ ·å¾—åˆ°çš„ç›®æ ‡å…³èŠ‚è§’
            angles = []

            # éšæœºåœ¨å…³èŠ‚ç©ºé—´é‡‡æ ·
            for joint_name in joints:
                joint_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_JOINT, joint_name)
                if joint_id == -1:
                    raise ValueError(f"Joint named '{joint_name}' not found in the model.")
                low_limit = self.model.jnt_range[joint_id, 0] if self.model.jnt_limited[joint_id] else -np.pi
                high_limit = self.model.jnt_range[joint_id, 1] if self.model.jnt_limited[joint_id] else np.pi
                random_angle = np.random.uniform(low_limit, high_limit)
                angles.append(random_angle)

            angles = np.array(angles)
            # 
            ori_qpos = self.data.qpos[:6].copy()
            # æ¨¡å‹å¾€å‰ä¸€æ­¥
            self.data.qpos[:6] = angles
            mujoco.mj_forward(self.model, self.data)
            mujoco.mj_step(self.model, self.data)
            goal_pos, goal_quat = self._get_site_pos_ori("end_ee")

            # æ¢å¤
            self.data.qpos[:6] = ori_qpos
            mujoco.mj_forward(self.model, self.data)
            mujoco.mj_step(self.model, self.data)

            x_goal, y_goal, z_goal = goal_pos[0], goal_pos[1], goal_pos[2]

            if (self.workspace_limits['x'][0] <= x_goal <= self.workspace_limits['x'][1] and
                self.workspace_limits['y'][0] <= y_goal <= self.workspace_limits['y'][1] and
                self.workspace_limits['z'][0] <= z_goal <= self.workspace_limits['z'][1]):

                goal_position = np.array([x_goal, y_goal, z_goal])
                self._label_goal_pose(goal_position, goal_quat)


                self.goal_pos = goal_position
                self.goal_quat = goal_quat
                self.goal_angle = angles
                return
    
    def map_action_to_joint_limits(self, action: np.ndarray) -> np.ndarray:
        """
        å°† [-1, 1] èŒƒå›´å†…çš„ action æ˜ å°„åˆ°æ¯ä¸ªå…³èŠ‚çš„å…·ä½“è§’åº¦èŒƒå›´ã€‚

        Args:
            action (np.ndarray): å½¢çŠ¶ä¸º (6,) çš„æ•°ç»„ï¼Œå€¼èŒƒå›´åœ¨ [-1, 1]

        Returns:
            np.ndarray: å½¢çŠ¶ä¸º (6,) çš„æ•°ç»„ï¼Œæ˜ å°„åˆ°å®é™…å…³èŠ‚è§’åº¦èŒƒå›´ï¼Œç±»å‹ä¸º numpy.ndarray
        """

        normalized = (action + 1) / 2
        lower_bounds = self.joint_limits[:, 0]
        upper_bounds = self.joint_limits[:, 1]
        # æ’å€¼è®¡ç®—
        mapped_action = lower_bounds + normalized * (upper_bounds - lower_bounds)

        return mapped_action
    
    def _set_state(self, qpos, qvel):
        assert qpos.shape == (6,) and qvel.shape == (6,)
        self.data.qpos[:6] = np.copy(qpos)
        self.data.qvel[:6] = np.copy(qvel)
        # mujoco ä»¿çœŸå‘å‰æ¨è¿›ä¸€æ­¥
        mujoco.mj_forward(self.model, self.data)
        mujoco.mj_step(self.model, self.data)

    def reset(self, seed=None, options=None):
        if seed is not None:
            self.seed(seed)
        noise_low = -self._reset_noise_scale
        noise_high = self._reset_noise_scale
        qpos = self.init_qpos + self.np_random.uniform(
            low=noise_low, high=noise_high, size=6
        )
        qvel = self.init_qvel + self.np_random.uniform(
            low=noise_low, high=noise_high, size=6
        )
        
        self._set_state(qpos, qvel)
        self._set_goal_pose()
        obs = self._get_observation()
        self.step_number = 0

        self.goal_reached = False

        return obs, {}
    
    def _get_image_from_camera(self, w, h, camera_name):
        viewport = mujoco.MjrRect(0, 0, w, h)
        cam_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_CAMERA, camera_name)
        self.camera.id = cam_id
        mujoco.mjv_updateScene(
            self.model, self.data, mujoco.MjvOption(), 
            None, self.camera, mujoco.mjtCatBit.mjCAT_ALL, self.scene
        )
        mujoco.mjr_render(viewport, self.scene, self.context)
        rgb = np.zeros((h, w, 3), dtype=np.uint8)
        mujoco.mjr_readPixels(rgb, None, viewport, self.context)
        cv_image = cv2.cvtColor(np.flipud(rgb), cv2.COLOR_RGB2BGR)
        return cv_image
    
    def _get_observation(self):
        wrist_cam_image = self._get_image_from_camera(640, 480, "wrist")
        top_cam_image = self._get_image_from_camera(640, 480, "3rd")
        obs = {
            "wrist": wrist_cam_image,
            "3rd": top_cam_image
        }
        return obs
    
    def _compute_pos_error_and_reward(self, cur_pos, goal_pos):
        # è®¡ç®—ä½ç½®è¯¯å·®ï¼ˆæ¬§æ°è·ç¦»ï¼‰
        pos_error = np.linalg.norm(cur_pos - goal_pos)
        pos_reward = -np.arctan(pos_error)
        return pos_reward, pos_error
    
    def _compute_ori_error_and_reward(self, cur_quat, goal_quat, axis_weight=None, use_arctan=True):
        """
        Compute orientation reward based on quaternion difference.

        Args:
            cur_quat (np.array): current orientation [w, x, y, z]
            goal_quat (np.array): goal orientation [w, x, y, z]
            axis_weight (np.array or None): weights for [rx, ry, rz] axes
            use_arctan (bool): whether to apply arctangent smoothing to reward

        Returns:
            float: orientation reward (the more negative, the worse the orientation mismatch)
        """

        # Convert to scipy Rotation objects
        r_current = Rotation.from_quat([cur_quat[1], cur_quat[2], cur_quat[3], cur_quat[0]])  # x,y,z,w
        r_goal = Rotation.from_quat([goal_quat[1], goal_quat[2], goal_quat[3], goal_quat[0]])  # x,y,z,w

        # è®¡ç®—ç›¸å¯¹æ—‹è½¬ï¼ˆå§¿æ€å·®ï¼‰âˆˆ [0, Ï€]
        r_diff = r_goal * r_current.inv()

        # Convert to rotation vector âˆˆ [[-Ï€, Ï€], [-Ï€, Ï€], [-Ï€, Ï€]]
        rotvec = r_diff.as_rotvec()  # shape (3,), angle * axis

        if axis_weight is None:
            # Default: equal weights
            axis_weight = np.array([1.0, 1.0, 1.0])

        # weighted_error âˆˆ [0, Ï€ â‹… 1.732]
        weighted_error = np.sum(axis_weight * np.abs(rotvec))

        # Optional: apply arctan smoothing
        if use_arctan:
            # orientation_reward âˆˆ [-1.387, 0]
            orientation_reward = -np.arctan(weighted_error)
        else:
            # orientation_reward âˆˆ [-Ï€ â‹… 1.732, 0]
            orientation_reward = -weighted_error

        return orientation_reward, weighted_error
    
    def _compute_reward(self, observation):
        # æå–å½“å‰æœ«ç«¯ pose
        cur_gripper_pos = observation[:3].copy()
        cur_gripper_quat = observation[3:7].copy()

        # ç›®æ ‡ pose
        goal_pos = self.goal_pos.copy()
        goal_quat = self.goal_quat.copy()

        # è®¡ç®—ä½ç½®è¯¯å·®ä¸ä½ç½®reward
        pos_reward, pos_error = self._compute_pos_error_and_reward(cur_gripper_pos, goal_pos)
        # è®¡ç®—å§¿æ€è¯¯å·® reward
        ori_reward, ori_error = self._compute_ori_error_and_reward(cur_gripper_quat, goal_quat)

        # ç»¼åˆå¥–åŠ±ï¼Œç»™å„ä¸ªrewardè®¾ç½®æƒé‡
        w_pos =  2.0
        w_ori = 0.2
        reward = w_pos * pos_reward + w_ori * ori_reward

        # è¾¾åˆ°ç›®æ ‡é˜ˆå€¼æ—¶ï¼Œå¢åŠ ç²¾ç»†å¥–åŠ±
        pos_thresh = 0.1  # 10 cm
        ori_thresh = 1.047
        # ç²¾ç»†å¥–åŠ±çš„æœ‰æ•ˆèŒƒå›´
        pos_range = 0.1     # 10 cm
        ori_range = 1.047   # 60Â° 

        success_pos_thresh = 0.05   # 5 cm
        success_ori_thresh = 0.2   # 11.5Â°

        if pos_error < pos_thresh and ori_error < ori_thresh:

            pos_fine_reward = 1.0 - np.tanh(pos_error / pos_range)
            ori_fine_reward = 1.0 - np.tanh(ori_error / ori_range)
            

            # ä½ç½®è¯¯å·®å·²ç»è¾ƒå°çš„æ—¶å€™, ä¼˜å…ˆå¥–åŠ±æ—‹è½¬
            w_fine_pos = 1.0
            w_fine_ori = 1.0

            fine_reward = w_fine_pos * pos_fine_reward + w_fine_ori * ori_fine_reward
            reward += fine_reward
            # è®¤ä¸ºåŸºæœ¬ä¸Šå·²ç»å®Œç¾åˆ°è¾¾ç›®æ ‡, å†å¢åŠ ä¸€éƒ¨åˆ†å¥–åŠ±
            if pos_error < success_pos_thresh and ori_error < success_ori_thresh:
                self.goal_reached = True
                reward += 10.0 
        return reward

    def step(self, action):
        # å°† action æ˜ å°„å›çœŸå®æœºæ¢°è‡‚å…³èŠ‚ç©ºé—´
        mapped_action = self.map_action_to_joint_limits(action)
        self.data.qpos[:6] = mapped_action
        self._label_goal_pose(self.goal_pos, self.goal_quat)
        # mujoco ä»¿çœŸå‘å‰æ¨è¿›ä¸€æ­¥ (è¿™é‡Œåªæ›´æ–° qpos , å¹¶ä¸ä¼šåšåŠ¨åŠ›å­¦ç§¯åˆ†)
        mujoco.mj_step(self.model, self.data)

        self.step_number += 1
        observation = self._get_observation()
        # æ£€æŸ¥è§‚æµ‹é‡æ˜¯å¦åŒ…å«æ— æ•ˆå€¼
        is_finite = np.isfinite(observation).all()

        # è®¡ç®— reward
        reward = self._compute_reward(observation)
        done = not is_finite or self.goal_reached
        info = {'is_success': done}

        # æ£€æŸ¥æ˜¯å¦æå‰ç»ˆæ­¢å½“å‰ç¯å¢ƒé‡‡æ ·
        truncated = self.step_number > self.episode_len
        if self.handle is not None:
            self.handle.sync()

        return observation, reward, done, truncated, info

    def seed(self, seed=None):
        self.np_random = np.random.default_rng(seed)
        return [seed]


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run the PiperEnv RL simulation.")
    parser.add_argument('--render', action='store_true', help='Enable rendering with GUI viewer')
    parser.add_argument('--n_envs', type=int, default=1, help='Number of envs')
    args = parser.parse_args()

    # âœ… å¢åŠ æ£€æŸ¥é€»è¾‘
    if args.render and args.n_envs != 1:
        raise ValueError("Rendering is only supported with --n_envs=1")

    # åˆ›å»º agent äº¤äº’ç¯å¢ƒ
    env = make_vec_env(lambda: PiperEnv(render=args.render), n_envs=args.n_envs)

    policy_kwargs = dict(
        activation_fn=nn.ReLU,
        net_arch=[dict(pi=[256, 128], vf=[256, 128])]
    )

    """
    è®­ç»ƒå‚æ•°
    å‚æ•°å	                   è§£é‡Š	                                å»ºè®®ä¸æ³¨æ„äº‹é¡¹
    learning_rate	          å­¦ä¹ ç‡	                           å›ºå®šå€¼é€‚åˆèµ·æ­¥ï¼Œè°ƒ schedule å¯æå‡ç¨³å®šæ€§
    n_steps	                  æ¯ä¸ªç¯å¢ƒæ¯æ¬¡ rollout çš„æ­¥æ•°            å¿…é¡»æ»¡è¶³ n_steps * n_envs > 1, æ¨èè®¾ä¸º 128~2048
    batch_size	              æ¯æ¬¡ä¼˜åŒ–çš„æœ€å° batch å¤§å°	             
    n_epochs	              æ¯æ¬¡æ›´æ–°é‡å¤è®­ç»ƒçš„æ¬¡æ•°	              å¢åŠ æ ·æœ¬åˆ©ç”¨ç‡ï¼Œ 3~10 æ˜¯å¸¸ç”¨åŒºé—´
    gamma	                  å¥–åŠ±æŠ˜æ‰£å› å­ï¼ˆé•¿æœŸ vs çŸ­æœŸï¼‰	          0.95~0.99 ä¹‹é—´ï¼Œä»»åŠ¡é•¿æœŸæ€§è¶Šå¼ºè®¾å¾—è¶Šé«˜
    device	                  è®­ç»ƒä½¿ç”¨è®¾å¤‡	                        GPU / CPU
    tensorboard_log           è®­ç»ƒæ—¥å¿—ä¿å­˜åœ°å€
    """
    model = PPO(
        "MultiInputPolicy",
        env,
        policy_kwargs=policy_kwargs,
        verbose=1,
        n_steps=10,
        batch_size=500,
        n_epochs=10,
        gamma=0.99,
        learning_rate=3e-4,
        device="cuda" if torch.cuda.is_available() else "cpu",
        tensorboard_log="./ppo_piper/"
    )

    """
    å‚æ•°å	                  è§£é‡Š	                                   
    total_timesteps          æ€»å…±ä¸ç¯å¢ƒäº¤äº’çš„æ­¥æ•° (env.step() çš„æ¬¡æ•°æ€»å’Œï¼‰   
    progress_bar             æ˜¯å¦æ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
    """
    model.learn(total_timesteps=2000*10000, progress_bar=True)
    model.save("piper_ik_ppo_model")

    print(" model sava success ! ")

    ### ç»§ç»­è®­ç»ƒ
    # model = PPO.load("./piper_ppo_model.zip")
    # model.set_env(env)
    # model.learn(total_timesteps=2048*100, progress_bar=Truget_action_and_value)


    """
    ğŸ” rollout/ éƒ¨åˆ†ï¼ˆç¯å¢ƒäº¤äº’ç»“æœï¼‰
    ep_len_mean	           æ¯ä¸ª episode å¹³å‡çš„æ­¥æ•° (æœ¬ä¾‹ä¸º 200)
    ep_rew_mean	           æ¯ä¸ª episode å¹³å‡çš„ç´¯è®¡ reward
    success_rate	       æ¯ä¸ª episode æ˜¯å¦å®ŒæˆæˆåŠŸä»»åŠ¡çš„æ¯”ä¾‹

    â± time/ éƒ¨åˆ†ï¼ˆè®­ç»ƒæ—¶é—´ç›¸å…³ï¼‰
    fps	                   æ¯ç§’ä»¿çœŸå¤šå°‘æ­¥
    iterations	           ç®—æ³•å·²å®Œæˆçš„ä¼˜åŒ–å‘¨æœŸ
    time_elapsed	       æ€»å…±è®­ç»ƒçš„æ—¶é—´, å•ä½æ˜¯ç§’
    total_timesteps	       æ€»å…±é‡‡æ ·è¿‡çš„ç¯å¢ƒæ­¥æ•°

    ğŸ¯ train/ éƒ¨åˆ†ï¼ˆç­–ç•¥å­¦ä¹ è´¨é‡
    approx_kl	           å½“å‰ç­–ç•¥å’Œæ—§ç­–ç•¥ä¹‹é—´çš„ KL æ•£åº¦ï¼ˆè¡¡é‡å˜åŒ–å¹…åº¦ï¼‰åˆç†èŒƒå›´çº¦ 0.01~0.03
    clip_fraction	       æœ‰å¤šå°‘åŠ¨ä½œè¢« clip_range é™åˆ¶ (PPO çš„æ ¸å¿ƒ) è¾ƒé«˜è¡¨ç¤ºè®­ç»ƒæ³¢åŠ¨å¤§
    clip_range	           PPO çš„è¶…å‚æ•°ï¼Œå¸¸è§é»˜è®¤å€¼ä¸º 0.2
    entropy_loss	       ç­–ç•¥åˆ†å¸ƒçš„ç†µï¼ˆæ¢ç´¢æ€§ï¼‰ã€‚è¶Šå¤§è¶Šéšæœºï¼Œè¶Šå°è¶Šç¡®å®š
    explained_variance	   Critic çš„é¢„æµ‹å€¼å’ŒçœŸå® return çš„ç›¸å…³æ€§ 1 è¡¨ç¤ºå®Œå…¨æ‹Ÿåˆï¼Œ< 0 è¡¨ç¤ºé¢„æµ‹å¾ˆå·®
    learning_rate	       å½“å‰å­¦ä¹ ç‡
    loss	               æ€»æŸå¤± (å€¼å‡½æ•° + ç­–ç•¥ + entropy)
    n_updates	           æ€»å…±ä¼˜åŒ–äº†å¤šå°‘æ¬¡
    policy_gradient_loss   ç­–ç•¥ç½‘ç»œçš„æ¢¯åº¦æŸå¤± (è´Ÿå€¼æ˜¯æ­£å¸¸çš„)
    std	                   ç­–ç•¥ç½‘ç»œè¾“å‡ºçš„åŠ¨ä½œæ ‡å‡†å·®ï¼ˆè¡¨ç¤ºç­–ç•¥ä¸ç¡®å®šæ€§ï¼‰
    value_loss	           Critic çš„å›å½’æŸå¤±ï¼Œè¶Šä½è¶Šå¥½
    """
