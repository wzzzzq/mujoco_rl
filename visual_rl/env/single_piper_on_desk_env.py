import numpy as np
import mujoco
import gym
from gym import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import torch.nn as nn
import warnings
import torch
import mujoco.viewer
import os
from scipy.spatial.transform import Rotation as Rotation
from stable_baselines3.common.evaluation import evaluate_policy
import argparse
import cv2
import glfw

# å¿½ç•¥ç‰¹å®šè­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="stable_baselines3.common.on_policy_algorithm") # å¿½ç•¥ stable_baselines3 ä¸­æŸäº›æ— å®³çš„è­¦å‘Šï¼Œé¿å…è¾“å‡ºå¹²æ‰°

class PiperEnv(gym.Env):
    """
    Piper robot arm environment for apple grasping task using delta (incremental) actions.
    
    The neural network outputs delta joint angles [Î´q1, Î´q2, Î´q3, Î´q4, Î´q5, Î´q6, Î´q7] 
    representing incremental changes to joint positions rather than absolute positions.
    This approach provides smoother control and better learning stability.
    """
    def __init__(self, render=False):
        super(PiperEnv, self).__init__()
        script_dir = os.path.dirname(os.path.realpath(__file__
        )) # è·å–å½“å‰è„šæœ¬æ–‡ä»¶æ‰€åœ¨ç›®å½•
        xml_path = os.path.join(script_dir, '../..', 'model_assets', 'piper_on_desk', 'scene.xml') # scene.xml æ¨¡å‹æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ï¼Œç”¨äºåŠ è½½ MuJoCo æ¨¡å‹

        self.model = mujoco.MjModel.from_xml_path(xml_path) # åŒ…å«æ¨¡å‹çš„é™æ€ä¿¡æ¯(å¦‚å‡ ä½•ç»“æ„ã€å…³èŠ‚å‚æ•°ç­‰)
        self.data = mujoco.MjData(self.model) # ä¿å­˜æ¨¡å‹çš„åŠ¨æ€çŠ¶æ€(å¦‚ä½ç½®ã€é€Ÿåº¦ã€åŠ›ç­‰)
        self.end_effector_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, 'link6') # æœ«ç«¯æ‰§è¡Œå™¨(å¤¹çˆª) ID

        self.render_mode = render # æ˜¯å¦å¯ç”¨å®æ—¶æ¸²æŸ“çª—å£
        if self.render_mode:
            self.handle = mujoco.viewer.launch_passive(self.model, self.data) # åˆ›å»ºä¸€ä¸ªè¢«åŠ¨æ¸²æŸ“çª—å£(GUI)ï¼Œå¯ä»¥å®æ—¶æŸ¥çœ‹ä»¿çœŸè¿‡ç¨‹
            self.handle.cam.distance = 3 # ç›¸æœºä¸ç›®æ ‡çš„è·ç¦»ä¸º 3
            self.handle.cam.azimuth = 0 # æ–¹ä½è§’ä¸º 0 åº¦
            self.handle.cam.elevation = -30 # ä»°è§’ä¸º -30 åº¦
        else:
            self.handle = None

        # å„å…³èŠ‚è¿åŠ¨é™ä½ï¼Œä¸€å…± 7 ä¸ªè‡ªç”±åº¦ï¼Œå‰ 6 ä¸ªæ˜¯æ—‹è½¬å…³èŠ‚ï¼Œæœ€åä¸€ä¸ªæ˜¯å¤¹çˆªå¼€åˆ
        self.joint_limits = np.array([
            (-2.618, 2.618),
            (0, 3.14),
            (-2.697, 0),
            (-1.832, 1.832),
            (-1.22, 1.22),
            (-3.14, 3.14),
            (0, 0.035),
        ])

        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(7,)) # åŠ¨ä½œç©ºé—´: æ¯ä¸ªå…³èŠ‚çš„è§’åº¦å¢é‡ [-1,1]
        self.observation_space = spaces.Dict({
            "3rd": spaces.Box(low=0, high=255, shape=(3, 480, 640), dtype=np.uint8) 
        }) 
        self.np_random = None # ç”¨äºç”Ÿæˆéšæœºæ•°ï¼Œå°†åœ¨ reset(seed=...) ä¸­åˆå§‹åŒ–
        self.step_number = 0 # è®°å½•å½“å‰ episode çš„æ­¥æ•°

        self.workspace_limits = {
            'x' : (0.1, 0.7),
            'y' : (-0.7, 0.7),
            'z' : (0.1, 0.7)
        } # å®šä¹‰æœºæ¢°è‡‚æœ«ç«¯å…è®¸åˆ°è¾¾çš„ç©ºé—´èŒƒå›´

        self.goal_reached = False # æ ‡è®°ç›®æ ‡æ˜¯å¦å·²è¾¾æˆ
        self._reset_noise_scale = 0.0 # åˆå§‹çŠ¶æ€æ‰°åŠ¨çš„å™ªå£°å¹…åº¦ï¼Œå¯ç”¨äºå¢åŠ è®­ç»ƒå¤šæ ·æ€§

        # åˆå§‹åŒ–ç›®æ ‡ pose
        # self.goal_pos = None
        # self.goal_quat = None
        # self.goal_angle = None
        # self._set_goal_pose()

        self.episode_len = 300 # æ¯ä¸ª episode çš„æœ€å¤§æ­¥æ•°
        self.init_qpos = np.zeros(8) + 0.01 # åˆå§‹å…³èŠ‚ä½ç½®
        self.init_qvel = np.zeros(8) + 0.01 # åˆå§‹å…³èŠ‚é€Ÿåº¦

        # ä½¿ç”¨ GLFW åˆ›å»ºä¸€ä¸ªä¸å¯è§çš„ OpenGL çª—å£(ç”¨äºç¦»çº¿æ¸²æŸ“)
        glfw.init()
        glfw.window_hint(glfw.VISIBLE, glfw.FALSE)
        self.window = glfw.create_window(640, 480, "offscreen", None, None) # ç”¨äºæ¸²æŸ“å›¾åƒè€Œä¸æ˜¾ç¤º GUI ç•Œé¢
        glfw.make_context_current(self.window)

        self.camera = mujoco.MjvCamera() # åˆ›å»ºä¸€ä¸ªå›ºå®šç±»å‹çš„ç›¸æœºå¯¹è±¡ï¼Œç”¨äºä»æŒ‡å®šè§†è§’æ¸²æŸ“å›¾åƒ
        self.camera.type = mujoco.mjtCamera.mjCAMERA_FIXED 
        self.scene = mujoco.MjvScene(self.model, maxgeom=1000) # MuJoCo åœºæ™¯å¯¹è±¡ï¼Œç”¨äºç®¡ç†æ¸²æŸ“çš„å‡ ä½•ä½“
        self.context = mujoco.MjrContext(self.model, mujoco.mjtFontScale.mjFONTSCALE_150) # MuJoCo æ¸²æŸ“ä¸Šä¸‹æ–‡ï¼Œç”¨äºå®é™…ç»˜å›¾æ“ä½œ
        mujoco.mjr_setBuffer(mujoco.mjtFramebuffer.mjFB_OFFSCREEN, self.context) # è®¾ç½®æ¸²æŸ“ç¼“å†²åŒºä¸ºç¦»çº¿æ¸²æŸ“æ¨¡å¼(ä¸æ˜¾ç¤ºåˆ°å±å¹•)
    
    def _get_site_pos_ori(self, site_name: str) -> tuple[np.ndarray, np.ndarray]:
        site_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE, site_name)
        if site_id == -1:
            raise ValueError(f"æœªæ‰¾åˆ°åä¸º '{site_name}' çš„site")

        # ä½ç½®
        position = np.array(self.data.site(site_id).xpos) # site åœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ä½ç½® shape (3,); t_{w}

        # æ–¹å‘: MuJoCo å·²å­˜æˆ 9 å…ƒç´ å‘é‡ï¼Œæ— éœ€ reshape
        xmat = np.array(self.data.site(site_id).xmat) # site çš„æ—‹è½¬çŸ©é˜µ(æŒ‰è¡Œæ’åˆ—); shape (9,); R_{w, site}
        quaternion = np.zeros(4)
        mujoco.mju_mat2Quat(quaternion, xmat) # [w, x, y, z]

        return position, quaternion
    
    def _get_body_pose(self, body_name: str) -> np.ndarray:
        """
        é€šè¿‡ body åç§°è·å–å…¶ä½å§¿ä¿¡æ¯, è¿”å›ä¸€ä¸ª 7 ç»´å‘é‡
            :param body_name: body åç§°å­—ç¬¦ä¸²
            :return: 7 ç»´ numpy æ•°ç»„, æ ¼å¼ä¸º [x, y, z, w, x, y, z]
            :raises ValueError: å¦‚æœæ‰¾ä¸åˆ°æŒ‡å®šåç§°çš„body
        """
        body_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, body_name)

        if body_id == -1:
            raise ValueError(f"æœªæ‰¾åˆ°åä¸º '{body_name}' çš„body")
        
        # æå–ä½ç½®å’Œå››å…ƒæ•°å¹¶åˆå¹¶ä¸ºä¸€ä¸ª 7 ç»´å‘é‡
        position = np.array(self.data.body(body_id).xpos) # [x, y, z]
        quaternion = np.array(self.data.body(body_id).xquat) # [w, x, y, z]
        
        return position, quaternion

    # å°†å¼ºåŒ–å­¦ä¹  agent è¾“å‡ºçš„åŠ¨ä½œå€¼ action ä» [-1, 1] èŒƒå›´æ˜ å°„åˆ°å…³èŠ‚è§’åº¦å¢é‡
    def map_action_to_joint_deltas(self, action: np.ndarray) -> np.ndarray:
        """
        å°† [-1, 1] èŒƒå›´å†…çš„ action æ˜ å°„åˆ°å…³èŠ‚è§’åº¦å¢é‡ (delta angles)ã€‚
        Args:
            action (np.ndarray): å½¢çŠ¶ä¸º (7,) çš„æ•°ç»„ï¼Œå€¼èŒƒå›´åœ¨ [-1, 1]ï¼Œè¡¨ç¤ºå„å…³èŠ‚çš„å¢é‡æ–¹å‘å’Œå¤§å°
        Returns:
            np.ndarray: å½¢çŠ¶ä¸º (7,) çš„æ•°ç»„ï¼Œæ˜ å°„åˆ°å®é™…å…³èŠ‚è§’åº¦å¢é‡ï¼Œç±»å‹ä¸º numpy.ndarray
        """
        # å®šä¹‰æ¯ä¸ªå…³èŠ‚çš„æœ€å¤§å¢é‡æ­¥é•¿ (å¼§åº¦)
        max_delta_per_step = np.array([
            0.1,   # joint 1: Â±0.1 rad per step (~5.7 degrees)
            0.1,   # joint 2: Â±0.1 rad per step
            0.1,   # joint 3: Â±0.1 rad per step  
            0.1,   # joint 4: Â±0.1 rad per step
            0.1,   # joint 5: Â±0.1 rad per step
            0.1,   # joint 6: Â±0.1 rad per step
            0.2,   # gripper: Â±0.2 units per step
        ])
        
        # action èŒƒå›´ [-1, 1] ç›´æ¥æ˜ å°„åˆ°å¢é‡èŒƒå›´ [-max_delta, +max_delta]
        delta_action = action * max_delta_per_step
        
        return delta_action
    
    def apply_joint_deltas_with_limits(self, current_qpos: np.ndarray, delta_action: np.ndarray) -> np.ndarray:
        """
        å°†å¢é‡åŠ¨ä½œåº”ç”¨åˆ°å½“å‰å…³èŠ‚ä½ç½®ï¼ŒåŒæ—¶ç¡®ä¿ä¸è¶…å‡ºå…³èŠ‚é™åˆ¶
        Args:
            current_qpos: å½“å‰å…³èŠ‚ä½ç½® (7,)
            delta_action: å…³èŠ‚ä½ç½®å¢é‡ (7,)
        Returns:
            np.ndarray: åº”ç”¨å¢é‡åçš„æ–°å…³èŠ‚ä½ç½®ï¼Œé™åˆ¶åœ¨å…³èŠ‚èŒƒå›´å†…
        """
        # è®¡ç®—æ–°çš„å…³èŠ‚ä½ç½®
        new_qpos = current_qpos + delta_action
        
        # è·å–å…³èŠ‚é™åˆ¶
        lower_bounds = self.joint_limits[:, 0]
        upper_bounds = self.joint_limits[:, 1]
        
        # å°†æ–°ä½ç½®é™åˆ¶åœ¨å…³èŠ‚èŒƒå›´å†…
        new_qpos = np.clip(new_qpos, lower_bounds, upper_bounds)
        
        return new_qpos
    
    # è®¾ç½® MuJoCo æ¨¡å‹çš„çŠ¶æ€(ä½ç½® qpos å’Œé€Ÿåº¦ qvel)ï¼Œå¹¶æ¨è¿›ä¸€æ­¥ä»¿çœŸ
    def _set_state(self, qpos, qvel):
        assert qpos.shape == (8,) and qvel.shape == (8,)
        self.data.qpos[:8] = np.copy(qpos)
        self.data.qvel[:8] = np.copy(qvel)
        mujoco.mj_step(self.model, self.data) # mujoco ä»¿çœŸå‘å‰æ¨è¿›ä¸€æ­¥ï¼Œä½¿çŠ¶æ€ç”Ÿæ•ˆ


    def reset(self, seed=None, options=None):
        if seed is not None:
            self.seed(seed)
        # å®šä¹‰åœ¨åˆå§‹åŒ–çŠ¶æ€ä¸­åŠ å…¥å™ªå£°çš„ä¸Šä¸‹é™èŒƒå›´
        noise_low = -self._reset_noise_scale
        noise_high = self._reset_noise_scale


        ## TODO step 1 : æŠŠæœºæ¢°è‡‚å½’é›¶
        qpos = self.init_qpos + self.np_random.uniform(
            low=noise_low, high=noise_high, size=8
        )
        qvel = self.init_qvel + self.np_random.uniform(
            low=noise_low, high=noise_high, size=8
        )
        
        self._set_state(qpos, qvel) # è®¾ç½®æ¨¡å‹çŠ¶æ€å¹¶å‰è¿›ä¸€æ­¥ä»¿çœŸä½¿å…¶ç”Ÿæ•ˆ

        ## TODO step 2 : æŠŠäº¤äº’çš„ç‰©å“é‡æ–°æ”¾ç½®ä½ç½®
        self._reset_object_pose()
        
        obs = self._get_observation() # è·å–åˆå§‹è§‚æµ‹å€¼

        ## TODO ä»¥ä¸‹ä¸¤ä¸ªä¿æŒä¸åŠ¨
        self.step_number = 0 # åˆå§‹åŒ– step è®¡æ•°å™¨
        self.goal_reached = False # å¯ç”¨äºåˆ¤æ–­æ˜¯å¦å®Œæˆä»»åŠ¡

        return obs, {}
    
    def set_goal_pose(self, goal_body_name, position, quat_wxyz):
        # è®¾ç½® target çš„ä½å§¿
        # goal_body_name = "target"
        goal_body_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, goal_body_name)

        if goal_body_id == -1:
            raise ValueError(f"Body named '{goal_body_name}' not found in the model.")

        # è·å– joint ID å’Œ qpos èµ·å§‹ç´¢å¼•
        goal_joint_id = self.model.body_jntadr[goal_body_id]
        goal_qposadr = self.model.jnt_qposadr[goal_joint_id]

        # è®¾ç½®ä½å§¿
        if goal_qposadr + 7 <= self.model.nq:
            self.data.qpos[goal_qposadr: goal_qposadr + 3] = position
            self.data.qpos[goal_qposadr + 3: goal_qposadr + 7] = quat_wxyz
        else:
            print("[è­¦å‘Š] target çš„ qpos ç´¢å¼•è¶Šç•Œæˆ– joint è®¾ç½®æœ‰è¯¯")
    
    def _reset_object_pose(self):
        # å°†è‹¹æœéšæœºæ”¾ç½®åœ¨æ¡Œå­ä¸­å¿ƒçš„åœ†å½¢åŒºåŸŸå†…ï¼Œç¡®ä¿ç›¸æœºå§‹ç»ˆå¯è§
        item_name = "apple"
        self.target_position, item_quat = self._get_body_pose(item_name)
        
        # ----------------------------éšæœºç›®æ ‡ä½ç½®----------------------------
        # æ¡Œå­ä¸­å¿ƒä½ç½® (åŸºäº scene.xml ä¸­æ¡Œå­çš„ pos="0 0 0.73")
        desk_center_x = 0.0
        desk_center_y = 0.0
        
        # åœ¨æ¡Œé¢ä¸­å¿ƒçš„åœ†å½¢åŒºåŸŸå†…éšæœºæ”¾ç½®ï¼ŒåŠå¾„è®¾ä¸ºæ¡Œå­å®½åº¦çš„ä¸€åŠä»¥ç¡®ä¿åœ¨æ¡Œé¢èŒƒå›´å†…
        max_radius = 0.1
        
        # ä½¿ç”¨æåæ ‡ç”Ÿæˆéšæœºä½ç½®
        theta = np.random.uniform(0, 2 * np.pi)  # å®Œæ•´åœ†å½¢èŒƒå›´
        rho = max_radius * np.sqrt(np.random.uniform(0, 1))  # å‡åŒ€åˆ†å¸ƒåœ¨åœ†å†…
        
        x_world_target = rho * np.cos(theta) + desk_center_x
        y_world_target = rho * np.sin(theta) + desk_center_y

        self.target_position[0] = x_world_target
        self.target_position[1] = y_world_target
        self.target_position[2] = 0.768  # Set apple on the table surface
        
        self.set_goal_pose("apple", self.target_position, item_quat)

    # ä» MuJoCo ä¸­çš„æŸä¸ªæŒ‡å®šåç§°çš„ç›¸æœº(camera)æ¸²æŸ“å›¾åƒï¼Œå¹¶è¿”å›ä¸€ä¸ª NumPy æ•°ç»„å½¢å¼çš„ OpenCV å›¾åƒ(BGR æ ¼å¼)
    def _get_image_from_camera(self, w, h, camera_name):
        viewport = mujoco.MjrRect(0, 0, w, h) # å®šä¹‰äº†ä¸€ä¸ªçŸ©å½¢åŒºåŸŸï¼Œç”¨äºæ¸²æŸ“å›¾åƒï¼Œ(0, 0, w, h) è¡¨ç¤ºä»å·¦ä¸Šè§’ (0, 0) å¼€å§‹ï¼Œå®½ wï¼Œé«˜ h
        cam_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_CAMERA, camera_name) # ä½¿ç”¨ mj_name2id() é€šè¿‡ camera_name æŸ¥æ‰¾ç›¸æœº ID
        self.camera.fixedcamid = cam_id # è®¾ç½® fixedcamid è¡¨ç¤ºå½“å‰è¦æ¸²æŸ“çš„å›ºå®šç›¸æœº ID
        mujoco.mjv_updateScene(
            self.model, self.data, mujoco.MjvOption(), 
            None, self.camera, mujoco.mjtCatBit.mjCAT_ALL, self.scene
        ) # æ„å»ºæ¸²æŸ“åœºæ™¯æ•°æ®åˆ° self.scene ä¸­
        mujoco.mjr_render(viewport, self.scene, self.context) # åœ¨æŒ‡å®šçš„ viewport ä¸Šä½¿ç”¨ self.scene å’Œ OpenGL ä¸Šä¸‹æ–‡ self.context è¿›è¡Œæ¸²æŸ“
        rgb = np.zeros((h, w, 3), dtype=np.uint8) # åˆ›å»ºä¸€ä¸ªç©ºçš„ RGB å›¾åƒæ•°ç»„ï¼Œå¤§å°ä¸º h x w x 3
        mujoco.mjr_readPixels(rgb, None, viewport, self.context) # å°†æ¸²æŸ“ç»“æœè¯»å…¥ rgb æ•°ç»„ä¸­ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯æ·±åº¦å›¾(è¿™é‡Œä¼  None è¡¨ç¤ºä¸éœ€è¦)
        cv_image = cv2.cvtColor(np.flipud(rgb), cv2.COLOR_RGB2BGR) # MuJoCo æ¸²æŸ“å‡ºæ¥çš„å›¾åƒä¸Šä¸‹é¢ å€’ï¼Œéœ€è¦ç”¨ np.flipud ç¿»è½¬å›æ¥; cv2.cvtColor(..., COLOR_RGB2BGR): å°† RGB è½¬æ¢ä¸º BGRï¼Œé€‚é… OpenCV é»˜è®¤æ ¼å¼
        return cv_image
    

    def _get_observation(self):
        wrist_cam_image = self._get_image_from_camera(640, 480, "wrist_cam")
        top_cam_image = self._get_image_from_camera(640, 480, "3rd")
        # å°†å›¾åƒä» (H, W, C) è½¬æ¢ä¸º (C, H, W)
        wrist_cam_image = np.transpose(wrist_cam_image, (2, 0, 1)) # æ–°å½¢çŠ¶: (3, 480, 640)
        top_cam_image = np.transpose(top_cam_image, (2, 0, 1)) # æ–°å½¢çŠ¶: (3, 480, 640)
        obs = {
            "3rd": top_cam_image
        }
        return obs
    

    def _check_contact_between_bodies(self, body1_name: str, body2_name: str) -> tuple[bool, float]:
        """
        æ£€æŸ¥ä¸¤ä¸ª body ä¹‹é—´æ˜¯å¦æœ‰æ¥è§¦ï¼Œå¹¶è¿”å›æ¥è§¦åŠ›çš„å¤§å°
        Args:
            body1_name: ç¬¬ä¸€ä¸ªbodyçš„åç§°
            body2_name: ç¬¬äºŒä¸ªbodyçš„åç§°
        Returns:
            tuple: (æ˜¯å¦æ¥è§¦, æ¥è§¦åŠ›å¤§å°)
        """
        # è·å– body ID
        body1_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, body1_name)
        body2_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, body2_name)
        
        if body1_id == -1 or body2_id == -1:
            return False, 0.0
            
        # éå†æ‰€æœ‰æ¥è§¦ç‚¹
        total_force = 0.0
        contact_found = False
        
        for i in range(self.data.ncon):
            contact = self.data.contact[i]
            
            # è·å–æ¥è§¦çš„ä¸¤ä¸ª geom
            geom1_id = contact.geom1
            geom2_id = contact.geom2
            
            # è·å–è¿™äº› geom æ‰€å±çš„ body
            geom1_body = self.model.geom_bodyid[geom1_id]
            geom2_body = self.model.geom_bodyid[geom2_id]
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬æ„Ÿå…´è¶£çš„ä¸¤ä¸ª body ä¹‹é—´çš„æ¥è§¦
            if ((geom1_body == body1_id and geom2_body == body2_id) or 
                (geom1_body == body2_id and geom2_body == body1_id)):
                contact_found = True
                
                # è®¡ç®—æ¥è§¦åŠ›å¤§å° (æ³•å‘åŠ›)
                contact_force = np.zeros(6)
                mujoco.mj_contactForce(self.model, self.data, i, contact_force)
                force_magnitude = np.linalg.norm(contact_force[:3])  # åªè€ƒè™‘æ³•å‘åŠ›
                total_force += force_magnitude
                
        return contact_found, total_force

    def _check_gripper_contact_with_object(self, object_name: str) -> tuple[bool, float]:
        """
        æ£€æŸ¥å¤¹çˆª(åŒ…æ‹¬ä¸¤ä¸ªæ‰‹æŒ‡)æ˜¯å¦ä¸æŒ‡å®šç‰©ä½“æ¥è§¦
        Args:
            object_name: ç‰©ä½“åç§°
        Returns:
            tuple: (æ˜¯å¦æ¥è§¦, æ€»æ¥è§¦åŠ›)
        """
        # æ£€æŸ¥ä¸¤ä¸ªæ‰‹æŒ‡æ˜¯å¦ä¸ç‰©ä½“æ¥è§¦
        finger1_contact, finger1_force = self._check_contact_between_bodies("link7", object_name)
        finger2_contact, finger2_force = self._check_contact_between_bodies("link8", object_name)
        
        # ä»»ä¸€æ‰‹æŒ‡æ¥è§¦å³è®¤ä¸ºå¤¹çˆªæ¥è§¦
        gripper_contact = finger1_contact or finger2_contact
        total_force = finger1_force + finger2_force
        
        return gripper_contact, total_force

    def _check_gripper_contact_with_table(self) -> tuple[bool, float]:
        """
        æ£€æŸ¥å¤¹çˆªæ˜¯å¦ä¸æ¡Œé¢æ¥è§¦
        Returns:
            tuple: (æ˜¯å¦æ¥è§¦, æ€»æ¥è§¦åŠ›)
        """
        # æ£€æŸ¥å¤¹çˆªå„éƒ¨åˆ†ä¸æ¡Œé¢çš„æ¥è§¦
        for i in range(1,9,1):
            link_name = f"link{i}"
            link_contact, link_force = self._check_contact_between_bodies(link_name, "desk")
            total_force = 0.0
            contact_found = False
            if link_contact:
                # å¦‚æœä»»ä¸€éƒ¨åˆ†æ¥è§¦ï¼Œè®°å½•æ¥è§¦ä¿¡æ¯
                contact_found = True
                # print(f"å¤¹çˆªä¸æ¡Œé¢æ¥è§¦: {link_name} æ¥è§¦åŠ› {link_force}")
                total_force += link_force

        return contact_found, total_force

    def _check_apple_fell_off_table(self) -> bool:
        """
        æ£€æŸ¥è‹¹æœæ˜¯å¦ä»æ¡Œå­ä¸Šæ‰è½
        Returns:
            bool: æ˜¯å¦æ‰è½
        """
        apple_position, _ = self._get_body_pose('apple')
        x, y, z = apple_position
        
        # æ¡Œå­çš„è¾¹ç•Œ (ä» scene.xml è·å–: pos="0 0 0.73", size="0.3 0.6 0.01115")
        table_x_min, table_x_max = -0.3, 0.3
        table_y_min, table_y_max = -0.6, 0.6
        table_surface_height = 0.74115  # 0.73 + 0.01115
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ¡Œé¢ x,y èŒƒå›´å†…
        if x < table_x_min or x > table_x_max:
            return True

        if y < table_y_min or y > table_y_max:
            return True

        # æ£€æŸ¥æ˜¯å¦æ‰åˆ°æ¡Œé¢ä»¥ä¸‹ (ç»™ä¸€äº›å®¹é”™ç©ºé—´ï¼Œå¦‚æœè‹¹æœä½äºæ¡Œé¢5cmåˆ™è®¤ä¸ºæ‰è½)
        if z < table_surface_height - 0.05:
            return True

        return False

    def _compute_distance_to_rest_qpos(self):
        """è®¡ç®—å½“å‰å…³èŠ‚è§’åº¦ä¸åˆå§‹ä¼‘æ¯ä½ç½®çš„è·ç¦»"""
        current_qpos = self.data.qpos[:7]
        rest_qpos = self.init_qpos[:7]  # ä½¿ç”¨åˆå§‹åŒ–ä½ç½®ä½œä¸ºä¼‘æ¯ä½ç½®
        return np.linalg.norm(current_qpos - rest_qpos)

    def _compute_reward(self):
        # Stage 1: Reward for reaching and grasping the apple
        # Stage 2: If grasping, reward for returning to rest pose  
        # Penalty for touching the table throughout
        
        # è·å–å½“å‰æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿å’Œç›®æ ‡ç‰©ä½“ä½å§¿
        end_ee_position, _ = self._get_site_pos_ori("end_ee")
        apple_position, _ = self._get_body_pose('apple')
        
        # è®¡ç®—TCPåˆ°ç›®æ ‡ç‰©ä½“çš„è·ç¦»
        tcp_to_obj_dist = np.linalg.norm(end_ee_position - apple_position)
        
        # Stage 1: æ¥è¿‘å¥–åŠ± - ä½¿ç”¨tanhå‡½æ•°æä¾›å¹³æ»‘çš„è·ç¦»å¥–åŠ±
        reaching_reward = 1 - np.tanh(5 * tcp_to_obj_dist)
        
        # æ£€æŸ¥æ˜¯å¦æŠ“å–åˆ°ç‰©ä½“
        apple_contact, apple_force = self._check_gripper_contact_with_object('apple')
        is_grasped = 1.0 if apple_contact and apple_force > 0.5 else 0.0  # éœ€è¦ä¸€å®šçš„æ¥è§¦åŠ›æ‰ç®—æŠ“å–
        
        # åŸºç¡€å¥–åŠ±: æ¥è¿‘å¥–åŠ± + æŠ“å–å¥–åŠ±
        reward = reaching_reward + is_grasped
        
        # Stage 2: å¦‚æœå·²æŠ“å–ï¼Œå¥–åŠ±è¿”å›ä¼‘æ¯ä½ç½®
        if is_grasped > 0:
            distance_to_rest = self._compute_distance_to_rest_qpos()
            place_reward = np.exp(-2 * distance_to_rest)
            reward += place_reward
            
            if tcp_to_obj_dist < 0.03 and distance_to_rest < 0.2:
                self.goal_reached = True
        
        # æƒ©ç½šæ¥è§¦æ¡Œé¢
        table_contact, table_force = self._check_gripper_contact_with_table()
        touching_table = 1.0 if table_contact else 0.0
        reward -= 2 * touching_table
        
        # ä¿å­˜å¥–åŠ±ç»„ä»¶ç”¨äºè°ƒè¯•å’Œåˆ†æ
        self.reward_components = {
            'reaching_reward': reaching_reward,
            'is_grasped': is_grasped,
            'place_reward': place_reward * is_grasped if is_grasped > 0 else 0.0,
            'table_penalty': -2 * touching_table,
            'tcp_to_obj_dist': tcp_to_obj_dist,
            'distance_to_rest': self._compute_distance_to_rest_qpos() if is_grasped > 0 else 0.0,
            'total_reward': reward
        }
        
        return reward
        

    # æ¥æ”¶ä¸€ä¸ªåŠ¨ä½œ actionï¼Œæ‰§è¡Œä¸€æ­¥ç¯å¢ƒé€»è¾‘ï¼Œå¹¶è¿”å›è§‚æµ‹å€¼ã€å¥–åŠ±ã€ç»ˆæ­¢ä¿¡å·ç­‰ä¿¡æ¯
    def step(self, action):
        # å°† action æ˜ å°„ä¸ºå…³èŠ‚è§’åº¦å¢é‡
        delta_action = self.map_action_to_joint_deltas(action)
        
        # è·å–å½“å‰å…³èŠ‚ä½ç½®
        current_qpos = self.data.qpos[:7].copy()
        
        # åº”ç”¨å¢é‡å¹¶ç¡®ä¿åœ¨å…³èŠ‚é™åˆ¶å†…
        new_qpos = self.apply_joint_deltas_with_limits(current_qpos, delta_action)
        
        # è®¾ç½®æ–°çš„å…³èŠ‚ç›®æ ‡ä½ç½®
        self.data.ctrl[:7] = new_qpos
        
        for _ in range(200):
            mujoco.mj_step(self.model, self.data) # mujoco ä»¿çœŸå‘å‰æ¨è¿›ä¸€æ­¥ï¼Œæ­¤å¤„ä¼šåšåŠ¨åŠ›å­¦ç§¯åˆ†ï¼Œæ›´æ–°æ‰€æœ‰ç‰©ç†çŠ¶æ€(ä½ç½®ã€é€Ÿåº¦ã€æ¥è§¦åŠ›ç­‰)
            if self.handle is not None:
                self.handle.sync()
            
            current_qpos = self.data.qpos[:7].copy() # æ›´æ–°å½“å‰å…³èŠ‚ä½ç½®
            pos_err = np.linalg.norm(new_qpos - current_qpos) # è®¡ç®—æ–°æ—§å…³èŠ‚ä½ç½®çš„è¯¯å·®
            # print(f"Step {_+1}: Position error = {pos_err:.4f}") # æ‰“å°æ¯ä¸€æ­¥çš„è¯¯å·®
            if pos_err < 0.08:
                 break

        self.step_number += 1 # æ›´æ–°æ­¥æ•°è®¡æ•°å™¨
        observation = self._get_observation() # è·å–å½“å‰çŠ¶æ€è§‚æµ‹å€¼

        reward = self._compute_reward() # è®¡ç®— reward

        apple_fell = self._check_apple_fell_off_table()
        done = self.goal_reached or apple_fell # ä»»åŠ¡å®Œæˆæˆ–è‹¹æœæ‰è½åˆ™ç»“æŸå½“å‰ episode

        # åˆ›å»ºè¯¦ç»†çš„infoå­—å…¸ï¼ŒåŒ…å«å¥–åŠ±ç»„ä»¶ä¿¡æ¯
        info = {
            'is_success': done,
            'reward_components': self.reward_components.copy() if hasattr(self, 'reward_components') else {},
            'total_reward': reward,
            'step_number': self.step_number,
            'goal_reached': self.goal_reached,
            'current_qpos': current_qpos.copy(),  # æ·»åŠ å½“å‰å…³èŠ‚ä½ç½®ä¿¡æ¯
            'delta_action': delta_action.copy(),  # æ·»åŠ å¢é‡åŠ¨ä½œä¿¡æ¯
            'new_qpos': new_qpos.copy()          # æ·»åŠ æ–°å…³èŠ‚ä½ç½®ä¿¡æ¯
        } 

        truncated = self.step_number > self.episode_len # å¦‚æœå½“å‰æ­¥æ•°è¶…è¿‡äº†é¢„è®¾çš„æœ€å¤§æ­¥æ•°ï¼Œåˆ™ episode è¢«æˆªæ–­ï¼Œé€šå¸¸è¡¨ç¤ºæœªå®Œæˆä»»åŠ¡ä½†æ—¶é—´åˆ°äº†ã€‚
        
        return observation, reward, done, truncated, info

    def seed(self, seed=None):
        self.np_random = np.random.default_rng(seed)
        return [seed]


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run the PiperEnv RL simulation.")
    parser.add_argument('--render', action='store_true', help='Enable rendering with GUI viewer')
    parser.add_argument('--n_envs', type=int, default=1, help='Number of envs')
    args = parser.parse_args()

    # å¢åŠ æ£€æŸ¥é€»è¾‘
    if args.render and args.n_envs != 1:
        raise ValueError("Rendering is only supported with --n_envs=1")

    # åˆ›å»º agent äº¤äº’ç¯å¢ƒ
    env = make_vec_env(lambda: PiperEnv(render=args.render), n_envs=args.n_envs)

    policy_kwargs = dict(
        activation_fn=nn.ReLU,
        net_arch=[dict(pi=[256, 128], vf=[256, 128])]
    )

    """
    è®­ç»ƒå‚æ•°
    å‚æ•°å	                   è§£é‡Š	                                å»ºè®®ä¸æ³¨æ„äº‹é¡¹
    learning_rate	          å­¦ä¹ ç‡	                           å›ºå®šå€¼é€‚åˆèµ·æ­¥ï¼Œè°ƒ schedule å¯æå‡ç¨³å®šæ€§
    n_steps	                  æ¯ä¸ªç¯å¢ƒæ¯æ¬¡ rollout çš„æ­¥æ•°            å¿…é¡»æ»¡è¶³ n_steps * n_envs > 1, æ¨èè®¾ä¸º 128~2048
    batch_size	              æ¯æ¬¡ä¼˜åŒ–çš„æœ€å° batch å¤§å°	             
    n_epochs	              æ¯æ¬¡æ›´æ–°é‡å¤è®­ç»ƒçš„æ¬¡æ•°	              å¢åŠ æ ·æœ¬åˆ©ç”¨ç‡ï¼Œ 3~10 æ˜¯å¸¸ç”¨åŒºé—´
    gamma	                  å¥–åŠ±æŠ˜æ‰£å› å­ï¼ˆé•¿æœŸ vs çŸ­æœŸï¼‰	          0.95~0.99 ä¹‹é—´ï¼Œä»»åŠ¡é•¿æœŸæ€§è¶Šå¼ºè®¾å¾—è¶Šé«˜
    device	                  è®­ç»ƒä½¿ç”¨è®¾å¤‡	                        GPU / CPU
    tensorboard_log           è®­ç»ƒæ—¥å¿—ä¿å­˜åœ°å€
    """
    model = PPO(
        "MultiInputPolicy",
        env,
        policy_kwargs=policy_kwargs,
        verbose=1,
        n_steps=128,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        learning_rate=3e-4,
        device="cuda" if torch.cuda.is_available() else "cpu",
        tensorboard_log="./ppo_piper/"
    )

    """
    å‚æ•°å	                  è§£é‡Š	                                   
    total_timesteps          æ€»å…±ä¸ç¯å¢ƒäº¤äº’çš„æ­¥æ•° (env.step() çš„æ¬¡æ•°æ€»å’Œï¼‰   
    progress_bar             æ˜¯å¦æ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
    """
    model.learn(total_timesteps=50000000, progress_bar=True)
    model.save("piper_ik_ppo_model")

    print(" model sava success ! ")

    ### ç»§ç»­è®­ç»ƒ
    # model = PPO.load("./piper_ppo_model.zip")
    # model.set_env(env)
    # model.learn(total_timesteps=2048*100, progress_bar=Truget_action_and_value)


    """
    ğŸ” rollout/ éƒ¨åˆ†ï¼ˆç¯å¢ƒäº¤äº’ç»“æœï¼‰
    ep_len_mean	           æ¯ä¸ª episode å¹³å‡çš„æ­¥æ•° (æœ¬ä¾‹ä¸º 200)
    ep_rew_mean	           æ¯ä¸ª episode å¹³å‡çš„ç´¯è®¡ reward
    success_rate	       æ¯ä¸ª episode æ˜¯å¦å®ŒæˆæˆåŠŸä»»åŠ¡çš„æ¯”ä¾‹

    â± time/ éƒ¨åˆ†ï¼ˆè®­ç»ƒæ—¶é—´ç›¸å…³ï¼‰
    fps	                   æ¯ç§’ä»¿çœŸå¤šå°‘æ­¥
    iterations	           ç®—æ³•å·²å®Œæˆçš„ä¼˜åŒ–å‘¨æœŸ
    time_elapsed	       æ€»å…±è®­ç»ƒçš„æ—¶é—´, å•ä½æ˜¯ç§’
    total_timesteps	       æ€»å…±é‡‡æ ·è¿‡çš„ç¯å¢ƒæ­¥æ•°

    ğŸ¯ train/ éƒ¨åˆ†ï¼ˆç­–ç•¥å­¦ä¹ è´¨é‡
    approx_kl	           å½“å‰ç­–ç•¥å’Œæ—§ç­–ç•¥ä¹‹é—´çš„ KL æ•£åº¦ï¼ˆè¡¡é‡å˜åŒ–å¹…åº¦ï¼‰åˆç†èŒƒå›´çº¦ 0.01~0.03
    clip_fraction	       æœ‰å¤šå°‘åŠ¨ä½œè¢« clip_range é™åˆ¶ (PPO çš„æ ¸å¿ƒ) è¾ƒé«˜è¡¨ç¤ºè®­ç»ƒæ³¢åŠ¨å¤§
    clip_range	           PPO çš„è¶…å‚æ•°ï¼Œå¸¸è§é»˜è®¤å€¼ä¸º 0.2
    entropy_loss	       ç­–ç•¥åˆ†å¸ƒçš„ç†µï¼ˆæ¢ç´¢æ€§ï¼‰ã€‚è¶Šå¤§è¶Šéšæœºï¼Œè¶Šå°è¶Šç¡®å®š
    explained_variance	   Critic çš„é¢„æµ‹å€¼å’ŒçœŸå® return çš„ç›¸å…³æ€§ 1 è¡¨ç¤ºå®Œå…¨æ‹Ÿåˆï¼Œ< 0 è¡¨ç¤ºé¢„æµ‹å¾ˆå·®
    learning_rate	       å½“å‰å­¦ä¹ ç‡
    loss	               æ€»æŸå¤± (å€¼å‡½æ•° + ç­–ç•¥ + entropy)
    n_updates	           æ€»å…±ä¼˜åŒ–äº†å¤šå°‘æ¬¡
    policy_gradient_loss   ç­–ç•¥ç½‘ç»œçš„æ¢¯åº¦æŸå¤± (è´Ÿå€¼æ˜¯æ­£å¸¸çš„)
    std	                   ç­–ç•¥ç½‘ç»œè¾“å‡ºçš„åŠ¨ä½œæ ‡å‡†å·®ï¼ˆè¡¨ç¤ºç­–ç•¥ä¸ç¡®å®šæ€§ï¼‰
    value_loss	           Critic çš„å›å½’æŸå¤±ï¼Œè¶Šä½è¶Šå¥½
    """